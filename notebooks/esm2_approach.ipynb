{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb88d2c5-f7b1-49cd-9e45-df552f867a80",
   "metadata": {},
   "source": [
    "### Drawback of traditional ML approach: Kh√¥ng gi·ªØ ƒë∆∞·ª£c order meaning c·ªßa chu·ªói axit amin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac6754-31c2-4d28-9654-8867e3b45e60",
   "metadata": {},
   "source": [
    "#### Sol: s·ª≠ d·ª•ng m√¥ h√¨nh esm2 650M tham s·ªë ƒë·ªÉ embedding c√°c chu·ªói acid amin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa35b0-af98-41c1-89bc-6e426b6dd599",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69266e5f-6de0-4045-89bc-4f204724623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c681d99-6943-4c20-9dea-c2983627f3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87397ece-1f18-4462-8662-9444fcc2fd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/esm2_t33_650M_UR50D...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmModel(\n",
       "  (embeddings): EsmEmbeddings(\n",
       "    (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): EsmEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-32): 33 x EsmLayer(\n",
       "        (attention): EsmAttention(\n",
       "          (self): EsmSelfAttention(\n",
       "            (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (rotary_embeddings): RotaryEmbedding()\n",
       "          )\n",
       "          (output): EsmSelfOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (intermediate): EsmIntermediate(\n",
       "          (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "        )\n",
       "        (output): EsmOutput(\n",
       "          (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (pooler): EsmPooler(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (contact_head): EsmContactPredictionHead(\n",
       "    (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval() #read-only to save VRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fc7c8-dabb-4934-a152-6e57c5befb31",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f609ee-33d4-4bbf-b76a-669d20fe9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(fasta_path, save_name, batch_size=8, limit=None):\n",
    "    \"\"\"\n",
    "    Read fasta file -> Run through ESM2 -> output: .npy\n",
    "    save_name: output file\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    sequences = []\n",
    "\n",
    "    print(f\"Reading file: {fasta_path}\")\n",
    "    for i, record in enumerate(SeqIO.parse(fasta_path, \"fasta\")):\n",
    "        if limit and i >= limit: break\n",
    "\n",
    "        #clean id\n",
    "        pid = str(record.id)\n",
    "        if \"|\" in pid:\n",
    "            pid = pid.split(\"|\")[1]\n",
    "\n",
    "        ids.append(pid)\n",
    "        #esm2 limit 1024 token\n",
    "        sequences.append(str(record.seq)[:1022])\n",
    "\n",
    "    print(f\"{len(sequences)} Proteins\")\n",
    "\n",
    "    #batching\n",
    "    embeddings = []\n",
    "    print(\"Creating embeddings...\")\n",
    "\n",
    "    for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "        batch_seqs = sequences[i : i + batch_size]\n",
    "\n",
    "        #tokenize\n",
    "        inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        #mean pooling\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask, 1)\n",
    "        sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "        embeddings.append(mean_embeddings.cpu().numpy())\n",
    "\n",
    "    final_embeddings = np.vstack(embeddings)\n",
    "\n",
    "    np.save(f\"/workspace/data/Embeddings/{save_name}.npy\", final_embeddings)\n",
    "    np.save(f\"/workspace/data/Embeddings/{save_name}_ids.npy\", ids)\n",
    "\n",
    "    return ids, final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9cc34-a585-4965-924b-0e4bfa46cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fasta = \"/workspace/data/Train/train_sequences.fasta\"\n",
    "\n",
    "train_ids, X_train = extract_embeddings(\n",
    "    train_fasta, \n",
    "    save_name=\"train_650M\", \n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(f\"Shape X_train: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516fd8c-8525-4a68-aaef-2b7898aa4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#prepare labels\n",
    "train_terms = pd.read_csv(\"/workspace/data/Train/train_terms.tsv\", sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "\n",
    "train_ids_set = set(train_ids)\n",
    "train_terms_filtered = train_terms[train_terms[\"EntryID\"].isin(train_ids_set)]\n",
    "\n",
    "top_n = 1500\n",
    "top_terms = train_terms_filtered[\"term\"].value_counts().head(top_n).index.tolist()\n",
    "\n",
    "Y_matrix = train_terms_filtered[train_terms_filtered[\"term\"].isin(top_terms)] \\\n",
    "            .pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "Y_train = Y_matrix.reindex(train_ids).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b21e0-2840-473a-9924-d77f53b286cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c7e45-86be-492c-be7a-d371b03853f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RidgeClassifier(alpha=1.0)\n",
    "clf.fit(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df688e-8fca-4eae-af99-e1c77f6b01f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_val = clf.predict(X_val)\n",
    "score = f1_score(Y_val, Y_pred_val, average='micro')\n",
    "print(f\"Local F1-Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073505a7-5e08-4218-9324-aa702bb1858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell ch·∫©n ƒëo√°n l·ªói ---\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# 1. Ki·ªÉm tra xem m√¥ h√¨nh c√≥ d·ª± ƒëo√°n ra c√°i g√¨ kh√¥ng?\n",
    "print(f\"T·ªïng s·ªë m·∫´u trong t·∫≠p Val: {Y_val.shape[0]}\")\n",
    "print(f\"T·ªïng s·ªë nh√£n c·∫ßn d·ª± ƒëo√°n: {Y_val.shape[0] * Y_val.shape[1]}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng nh√£n 1 (Th·ª±c t·∫ø): {Y_val.sum()}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng nh√£n 1 (M√¥ h√¨nh d·ª± ƒëo√°n): {Y_pred_val.sum()}\")\n",
    "\n",
    "# 2. N·∫øu s·ªë d·ª± ƒëo√°n qu√° th·∫•p (g·∫ßn b·∫±ng 0), ta c·∫ßn h·∫° ng∆∞·ª°ng (Threshold)\n",
    "print(\"\\n--- Th·ª≠ ch·ªânh ng∆∞·ª°ng th·ªß c√¥ng ---\")\n",
    "# L·∫•y ƒëi·ªÉm s·ªë th√¥ thay v√¨ nh√£n c·ª©ng 0/1\n",
    "decision_scores = clf.decision_function(X_val) \n",
    "\n",
    "# Th·ª≠ c√°c ng∆∞·ª°ng kh√°c nhau\n",
    "for thr in [0, -0.5, -1.0]: # Ridge score c√≥ th·ªÉ √¢m\n",
    "    y_pred_new = (decision_scores > thr).astype(int)\n",
    "    new_f1 = f1_score(Y_val, y_pred_new, average='micro')\n",
    "    print(f\"Ng∆∞·ª°ng {thr}: F1-Score = {new_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df6d097d-5ca1-4459-a286-95445364e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 224309/224309 [1:46:48<00:00, 47.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "FASTA_PATH = \"/workspace/data/Test/testsuperset.fasta\" \n",
    "SAVE_DIR = \"/workspace/data/Embeddings/embeddings_chunks\"\n",
    "MODEL_NAME = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "CHUNK_SIZE = 5000  \n",
    "BATCH_SIZE = 8    \n",
    "\n",
    "# --- SETUP ---\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "def process_and_save(seqs, ids, part_idx):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(seqs), BATCH_SIZE):\n",
    "        batch_seqs = seqs[i : i + BATCH_SIZE]\n",
    "        inputs = tokenizer(batch_seqs, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            mask = inputs['attention_mask'].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * mask, 1)\n",
    "            sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "        embeddings.append(mean_embeddings.cpu().numpy())\n",
    "    \n",
    "    final_emb = np.vstack(embeddings)\n",
    "    np.save(f\"{SAVE_DIR}/test_part_{part_idx}.npy\", final_emb)\n",
    "    np.save(f\"{SAVE_DIR}/test_ids_{part_idx}.npy\", ids)\n",
    "\n",
    "sequences = []\n",
    "ids = []\n",
    "part_counter = 0\n",
    "\n",
    "pbar = tqdm(total=224309) \n",
    "\n",
    "for record in SeqIO.parse(FASTA_PATH, \"fasta\"):\n",
    "    save_path_check = f\"{SAVE_DIR}/test_part_{part_counter}.npy\"\n",
    "    \n",
    "    if os.path.exists(save_path_check):\n",
    "        sequences.append(1) \n",
    "        if len(sequences) >= CHUNK_SIZE:\n",
    "            sequences = [] \n",
    "            ids = []\n",
    "            part_counter += 1\n",
    "            pbar.update(CHUNK_SIZE)\n",
    "        continue\n",
    "\n",
    "    pid = str(record.id).split(\"|\")[1] if \"|\" in str(record.id) else str(record.id)\n",
    "    ids.append(pid)\n",
    "    sequences.append(str(record.seq)[:1022])\n",
    "    pbar.update(1)\n",
    "    \n",
    "    if len(sequences) >= CHUNK_SIZE: \n",
    "        process_and_save(sequences, ids, part_counter)\n",
    "        part_counter += 1\n",
    "        \n",
    "        sequences = []\n",
    "        ids = []\n",
    "        gc.collect()\n",
    "\n",
    "if len(sequences) > 0:\n",
    "    process_and_save(sequences, ids, part_counter)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ade8a3-f0ab-48ed-ba9a-2951ef9a5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "TRAIN_EMB_PATH = \"/workspace/data/Embeddings/train_650M.npy\" \n",
    "TRAIN_IDS_PATH = \"/workspace/data/Embeddings/train_650M_ids.npy\"\n",
    "TRAIN_TERMS_PATH = \"/workspace/data/Train/train_terms.tsv\"\n",
    "TEST_CHUNKS_DIR = \"/workspace/data/Embeddings/embeddings_chunks\"\n",
    "OUTPUT_FILE = \"submission_level4_FINAL_fixed.tsv\"\n",
    "\n",
    "# Re-training\n",
    "print(\"Loading data and training model...\")\n",
    "X_train = np.load(TRAIN_EMB_PATH)\n",
    "train_ids = np.load(TRAIN_IDS_PATH, allow_pickle=True)\n",
    "\n",
    "train_terms = pd.read_csv(TRAIN_TERMS_PATH, sep=\"\\t\", usecols=[\"EntryID\", \"term\"])\n",
    "top_n = 1500\n",
    "top_terms = train_terms[\"term\"].value_counts().head(top_n).index.tolist()\n",
    "train_ids_set = set(train_ids)\n",
    "train_terms_filtered = train_terms[train_terms[\"EntryID\"].isin(train_ids_set) & train_terms[\"term\"].isin(top_terms)]\n",
    "\n",
    "Y_matrix = train_terms_filtered.pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "Y_train = Y_matrix.reindex(train_ids).fillna(0).astype(int)\n",
    "terms_columns = Y_train.columns \n",
    "\n",
    "clf = RidgeClassifier(alpha=1.0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "del X_train, Y_train, train_terms, train_terms_filtered, Y_matrix\n",
    "gc.collect()\n",
    "\n",
    "print(\"Outputting (Top K)...\")\n",
    "\n",
    "chunk_files = sorted(glob.glob(f\"{TEST_CHUNKS_DIR}/test_part_*.npy\"), \n",
    "                     key=lambda x: int(x.split('_')[-1].replace('.npy','')))\n",
    "\n",
    "output_lines = []\n",
    "TOP_K = 50   \n",
    "THRESHOLD = 0.01 #increased threshold\n",
    "\n",
    "for f_path in tqdm(chunk_files):\n",
    "    X_chunk = np.load(f_path)\n",
    "    id_path = f_path.replace(\"test_part_\", \"test_ids_\")\n",
    "    ids_chunk = np.load(id_path, allow_pickle=True)\n",
    "    \n",
    "    #predicting\n",
    "    decision_scores = clf.decision_function(X_chunk)\n",
    "    probs = 1 / (1 + np.exp(-decision_scores))\n",
    "    \n",
    "    for i, pid in enumerate(ids_chunk):\n",
    "        prob_row = probs[i]\n",
    "        \n",
    "        # 1. thresholding\n",
    "        mask = prob_row > THRESHOLD\n",
    "        if not np.any(mask):\n",
    "            indices = np.argsort(prob_row)[-5:]\n",
    "        else:\n",
    "            candidates = np.where(mask)[0]\n",
    "            \n",
    "            #2: only select the top 50\n",
    "            if len(candidates) > TOP_K:\n",
    "                # get candidate scores\n",
    "                cand_probs = prob_row[candidates]\n",
    "                # sort for top k\n",
    "                top_k_local_idx = np.argsort(cand_probs)[-TOP_K:]\n",
    "                indices = candidates[top_k_local_idx]\n",
    "            else:\n",
    "                indices = candidates\n",
    "            \n",
    "        for idx in indices:\n",
    "            term = terms_columns[idx]\n",
    "            score = prob_row[idx]\n",
    "            output_lines.append(f\"{pid}\\t{term}\\t{score:.3f}\")\n",
    "            \n",
    "    del X_chunk, ids_chunk, decision_scores, probs\n",
    "    gc.collect()\n",
    "\n",
    "# Output file\n",
    "print(f\"üíæ Saving {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a3be4-23ae-472b-a210-f280f2013ff1",
   "metadata": {},
   "source": [
    "#### Score: 0.192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af3504-0f41-4ad2-9552-fa926369c9a6",
   "metadata": {},
   "source": [
    "## Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998d396-61f8-462b-9e2b-5163034c552a",
   "metadata": {},
   "source": [
    "### GO Hierarchy: Ridge classifier ƒëang h·ªçc c√°c nh√£n 1 c√°ch ƒë·ªôc l·∫≠p, nh√£n con c√≥ th·ªÉ c√≥ score cao, nh∆∞ng nh·ªØng nh√£n cha chung chung th√¨ score l·∫°i th·∫•p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0556a-def4-4de0-bbe2-5526eac4b75d",
   "metadata": {},
   "source": [
    "### Sol: Ensemble: Mix v·ªõi naive approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9861fdff-8d1b-4719-983a-02e2947cbcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ESM2 output file....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11215450it [00:07, 1548396.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file Naive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10093905it [00:05, 1690414.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17589919/17589919 [00:20<00:00, 844235.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving: /workspace/notebooks/submission_ensemble_boosted.tsv\n",
      "Finisehd.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "LEVEL4_FILE = \"/workspace/notebooks/submission_level4_FINAL_fixed.tsv\"\n",
    "NAIVE_FILE = \"/workspace/notebooks/submission_naive.tsv\"\n",
    "OUTPUT_FILE = \"/workspace/notebooks/submission_ensemble_boosted.tsv\"\n",
    "\n",
    "print(\"Reading ESM2 output file....\")\n",
    "preds_l4 = {}\n",
    "\n",
    "try:\n",
    "    with open(LEVEL4_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            # Key : (ProteinID, GO_Term)\n",
    "            key = (parts[0], parts[1])\n",
    "            preds_l4[key] = float(parts[2])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {LEVEL4_FILE}.\")\n",
    "    raise\n",
    "\n",
    "print(\"Reading file Naive...\")\n",
    "preds_naive = {}\n",
    "try:\n",
    "    with open(NAIVE_FILE) as f:\n",
    "        for line in tqdm(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3: continue\n",
    "            key = (parts[0], parts[1])\n",
    "            preds_naive[key] = float(parts[2])\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found {NAIVE_FILE}\")\n",
    "    raise\n",
    "\n",
    "print(\"Ensembling...\")\n",
    "\n",
    "#Select all id-term pairs in 2 files\n",
    "all_keys = set(preds_l4.keys()) | set(preds_naive.keys())\n",
    "output_lines = []\n",
    "\n",
    "W_L4 = 0.6\n",
    "W_NAIVE = 0.4\n",
    "\n",
    "for key in tqdm(all_keys):\n",
    "    pid, term = key\n",
    "    \n",
    "    # L·∫•y ƒëi·ªÉm s·ªë, n·∫øu file n√†o kh√¥ng c√≥ th√¨ coi l√† 0\n",
    "    score_l4 = preds_l4.get(key, 0.0)\n",
    "    score_naive = preds_naive.get(key, 0.0)\n",
    "    \n",
    "    # C√¥ng th·ª©c c·ªông g·ªôp\n",
    "    final_score = (score_l4 * W_L4) + (score_naive * W_NAIVE)\n",
    "    \n",
    "    # Ch·ªâ ghi nh·ªØng d√≤ng c√≥ ƒëi·ªÉm s·ªë > 0.001 ƒë·ªÉ file ƒë·ª° n·∫∑ng\n",
    "    if final_score > 0.001:\n",
    "        output_lines.append(f\"{pid}\\t{term}\\t{final_score:.3f}\")\n",
    "\n",
    "# Ghi ra file\n",
    "print(f\"Saving: {OUTPUT_FILE}\")\n",
    "with open(OUTPUT_FILE, \"w\") as f:\n",
    "    f.write(\"\\n\".join(output_lines))\n",
    "\n",
    "print(f\"Finisehd.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca5496-c9d6-42e9-8b27-bea6538afe82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
